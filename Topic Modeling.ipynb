{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x27BFWfwFPW"
   },
   "source": [
    "# *Natural Language Processing Assignment*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"teal\">**1. Basic preprocessing**</font>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9XMO0rukSUI"
   },
   "source": [
    "\n",
    "###### <font color=\"teal\">**1.1 Open the database. Generate simple statistics about the abstracts. How many unique articles are there? What is the mean length of abstracts in characters?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gXQcDhc1IuOI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique articles: 802\n",
      "Mean length of abstracts: 1496.18 characters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"PLOS_narrativity.csv\")\n",
    "\n",
    "unique_articles = data['pmid'].nunique()\n",
    "\n",
    "mean_abstract_length = data['ab'].dropna().apply(len).mean()\n",
    "\n",
    "print(f\"Unique articles: {unique_articles}\")\n",
    "print(f\"Mean length of abstracts: {mean_abstract_length:.2f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNv0YjyJ1rhc"
   },
   "source": [
    "##### <font color=\"teal\">**2. Word-level preprocessing**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTGT_vBp1uA0"
   },
   "source": [
    "###### <font color=\"teal\">**2.1 Split the abstracts into list of words. How many different words are there in the vocabulary?**</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in abstracts: 19383\n"
     ]
    }
   ],
   "source": [
    "all_abstracts = ' '.join(data['ab'].dropna())\n",
    "\n",
    "words = set(all_abstracts.lower().split(sep=' '))\n",
    "\n",
    "unique_word_count = len(words)\n",
    "\n",
    "print(f\"Unique words in abstracts: {unique_word_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDphQOEC2pUI"
   },
   "source": [
    "###### <font color=\"teal\">**2.2 Split the abstracts into list of words using three different tokenizers from nltk. What is the difference in terms of number of words? What do you think has changed?**</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, ToktokTokenizer, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenizer: 16576 unique words\n",
      "Toktok Tokenizer: 16594 unique words\n",
      "Tweet Tokenizer: 14432 unique words\n"
     ]
    }
   ],
   "source": [
    "all_abstracts = ' '.join(data['ab'].dropna())\n",
    "\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "toktok_tokenizer = ToktokTokenizer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "treebank_tokens = treebank_tokenizer.tokenize(all_abstracts)\n",
    "toktok_tokens = toktok_tokenizer.tokenize(all_abstracts)\n",
    "tweet_tokens = tweet_tokenizer.tokenize(all_abstracts)\n",
    "\n",
    "treebank_unique_words = set(treebank_tokens)\n",
    "toktok_unique_words = set(toktok_tokens)\n",
    "tweet_unique_words = set(tweet_tokens)\n",
    "\n",
    "print(f\"Treebank Tokenizer: {len(treebank_unique_words)} unique words\")\n",
    "print(f\"Toktok Tokenizer: {len(toktok_unique_words)} unique words\")\n",
    "print(f\"Tweet Tokenizer: {len(tweet_unique_words)} unique words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, for the split word by space, we got 19383. \n",
    "Treebank got 16576, Toktok got 16594, and Tweet got 14432.\n",
    "\n",
    "TreebankWordTokenizer:\n",
    "Style: Based on the Penn Treebank tokenization.\n",
    "Characteristics: Handles punctuation like periods and commas separately; splits contractions (e.g., \"can't\" -> \"ca n't\").\n",
    "\n",
    "ToktokTokenizer:\n",
    "Style: A general-purpose tokenizer.\n",
    "Characteristics: Maintains a balance between splitting and grouping; respects some punctuation.\n",
    "\n",
    "TweetTokenizer:\n",
    "Style: Designed for social media text.\n",
    "Characteristics: Keeps hashtags, mentions, and emoticons intact; better for informal language.\n",
    "\n",
    "The different between the number of the words is depends on how they handle words and punctuation, contractions, and special symbols.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alterations.\n",
      "chlorophyll\n",
      "affect.\n",
      "diminished\n",
      "indexing\n",
      "Future\n",
      "garter\n",
      "effect.\n",
      "linearly\n",
      "1,372\n",
      "compute\n",
      "complemented\n",
      "800-170\n",
      "noted\n",
      "theoretical\n",
      "camps\n",
      "line\n",
      "perspectives.\n",
      "List\n",
      "unpredictable\n"
     ]
    }
   ],
   "source": [
    "# print random sample of all tokenizer\n",
    "import random\n",
    "\n",
    "words_list = list(words)\n",
    "treebank_unique_words_list = list(treebank_unique_words)\n",
    "toktok_unique_words_list = list(toktok_unique_words)\n",
    "tweet_unique_words_list = list(tweet_unique_words)\n",
    "\n",
    "for i in range(20):\n",
    "    # print(words_list[random.randint(0, len(words_list) - 1)])\n",
    "    # print(treebank_unique_words_list[random.randint(0, len(treebank_unique_words_list) - 1)])\n",
    "    print(toktok_unique_words_list[random.randint(0, len(toktok_unique_words_list) - 1)])\n",
    "    # print(tweet_unique_words_list[random.randint(0, len(tweet_unique_words_list) - 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omVMvmwuBvDk"
   },
   "source": [
    "##### <font color=\"teal\">**3. Domain specificity and regex**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ6pLbSlBxpM"
   },
   "source": [
    "###### <font color=\"teal\">**3.1 Use regex to retrieve numbers (ints, floats, %, years, ...) in abstracts.**</font>\n",
    "\n",
    "\n",
    "*Regex cheasheet* : see python's re module documentation https://docs.python.org/3/library/re.html  \n",
    "\n",
    "*Other ressources* : \n",
    "\n",
    "- A good website to write and test regular expressions : \n",
    "https://regex101.com/\n",
    "- A good game to learn regex : https://alf.nu/RegexGolf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29883 numbers in the abstracts.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\b\\d+\\.\\d+|\\b\\d+%|\\b\\d{4}|\\b\\d+\\b'\n",
    "matches = re.findall(pattern, all_abstracts)\n",
    "print(f\"Found {len(matches)} numbers in the abstracts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"w\") as f:\n",
    "    abstr = data['ab'].head(10)\n",
    "    for i in range(len(abstr)):\n",
    "        f.write(abstr[i])\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OscHta8ZBz8E"
   },
   "source": [
    "###### <font color=\"teal\">**3.2 How many percent of characters are numbers (as defined above) in a given abstract?**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0.36 percentages in the abstracts.\n"
     ]
    }
   ],
   "source": [
    "characters = data['ab'].dropna().apply(len)\n",
    "\n",
    "percentages = len(matches)/characters.sum() * 100\n",
    "print(f\"Found {percentages:.2f} percentages in the abstracts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD3M-sfgb-X-"
   },
   "source": [
    "##### <font color=\"teal\">**4. Classic NLP pipeline**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9A6V5y1cKFu"
   },
   "source": [
    "###### <font color=\"teal\">**4.0 Re-tokenize using spacy**</font>\n",
    "\n",
    "It is useful to take a look at spacy's [tokenizer documentation](https://spacy.io/usage/spacy-101#annotations-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtzuSyQbcTBX"
   },
   "source": [
    "###### <font color=\"teal\">**4.1 Lemmatize using spacy**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0CN5Ls3cfFC"
   },
   "source": [
    "###### <font color=\"teal\">**4.2 POS tagging using spacy, plot the trees**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlcrJDASUxOd"
   },
   "source": [
    "###### <font color=\"teal\">**4.3 NER using spacy, give the amount of each entity type for a given abstract, visualize entities**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxbSYFbokF0G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2BCX2wSt9VA"
   },
   "source": [
    "\n",
    "##### <font color=\"teal\">**5. Topic Modelling**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqzXrw-Tt_0Y"
   },
   "source": [
    "###### <font color=\"teal\">**5.1 Use Gensim's LDA to compute a topic model.**</font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtaxXtE4uCPE"
   },
   "source": [
    "###### <font color=\"teal\">**5.2 Use PyLDAvis to visualise the topic model. What are the different topic clusters?**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font color=\"teal\">**5.3 Use a tf-idf representation for each abstract, and use your favorite clustering algorithm.**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10P6zNk-A3bq0XConhmKB59J9nZANyCTX",
     "timestamp": 1580810612679
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
